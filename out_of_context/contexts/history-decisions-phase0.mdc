---
name: history-decisions-phase0
type: history
phase: phase-0-tech-selection
timeframe: 2025-12-16
status: archived
created_at: 2025-12-16T18:00:00
---

# Decision History - Phase 0 (Planning & Technology Selection)

**Timeframe:** 2025-12-16 (Day 1)  
**Phase:** Phase 0 - Technology Selection & Design  
**Status:** Archived (reference only)

This context archives all non-critical decisions made during Phase 0 planning. For the critical database choice, see `decision-database-chromadb`.

---

## Workflow Revision - Action Items Resequenced

**Decision Date:** 2025-12-16 (afternoon session)

**Problem Identified:**
- Original workflow had design document before database selection
- Can't design architecture without knowing which database to use
- Design needs concrete technology stack, not abstract "some vector DB"

**Decision:**
Resequence action items to follow proper dependencies:
1. **Database Selection FIRST** (Dev 00) ✅ Complete
2. **Design Document** (Dev 01) - after DB choice
3. **Execution Plan** (Dev 02) - after design
4. **Gate 1 Review** - before implementation
5. **Implementation** (Dev 03-09) - after gate approval

**Rationale:**
- **Database is foundational** - affects schema design, APIs, performance characteristics
- **Design needs concrete context** - can't design with abstract placeholders
- **Realistic estimation** - task complexity depends on chosen technology
- **Risk reduction** - validate performance assumptions early (1 day POC vs weeks of wrong design)

**Impact:**
- Developer can start immediately with Dev 00 (no design document needed first)
- Design review happens with concrete technology choice (better feedback)
- Execution plan has realistic estimates based on actual design
- Total timeline unchanged (2 weeks to MVP)

**New Workflow Structure:**
See `action-items-revised` context for complete details.

**Workflow Version:** 2.0 (replaces original action items structure)

---

## Requirements Document (v2.0.0 - Revised)

**Decision Date:** 2025-12-16 morning

**Changes from v1.0:**
- Added Quick Reference section for developers
- Clarified MVP scope (semantic only, BM25 in Phase 2)
- Moved evidence to appendix for readability
- Reduced research tasks from 9 to 6
- Added evaluation methodology
- Separated interface requirements from implementation details

**Rationale:** Developer critique showed v1.0 mixed strategy (WHAT) with tactics (HOW), creating confusion about what's required vs what's design freedom.

---

## Evidence-First Methodology

**Decision:** Requirements derived from evidence, not narrative-first approach

**Key Evidence:**
- [ANT-2025] Anthropic Contextual Retrieval: 49% improvement (semantic), 67% (hybrid)
- Informs: Semantic search MUST, BM25 SHOULD, hybrid is Phase 2

**Application:** Each requirement cites evidence, explains derivation, states confidence level

---

## MVP Scope Definition

**In Scope (Phase 1):**
- ✅ Semantic search with local embeddings
- ✅ File synchronization (eventual consistency)
- ✅ Index rebuild capability

**NOT in MVP:**
- ❌ BM25 lexical search (Phase 2)
- ❌ Metadata filtering API (schema OK, API later)
- ❌ Chunking (full-document only)
- ❌ Hybrid search (requires BM25 first)

**Rationale:** Semantic alone provides 49% improvement (substantial value), BM25 adds 18% more (diminishing returns), prioritize semantic first per user constraint.

---

## Technology Constraints (Locked)

**Embedding Model:** all-MiniLM-L6-v2 via sentence-transformers
- Why: Balances speed (2,800 sent/sec), quality (68.06 SBERT), size (80MB, 384 dim)

**Vector Database:** ChromaDB (decided in Dev 00) ✅
- Why: 1.9x faster at max capacity, built-in hybrid search, ephemeral client for STDIO
- See: `decision-database-chromadb` for full analysis

**Consistency Model:** Eventual (< 10s convergence target)
- Why: Enables async updates, simpler error handling, acceptable for use case

**Scale Target:** 500k-1M tokens (1K-2K contexts), future 2M tokens
- Why: Matches LLM context windows (200k-1M typical)

**Transport:** Self-contained, STDIO only
- Why: MCP library constraint, no external services allowed

---

## Action Items Structure

**Decision:** Separate scientist and developer action items into phases

**Original Structure:**
```
docs/v1/database/
├── scientist/ (3 action items, 8-12h total)
└── developer/ (5 action items, 4-6 days total)
```

**Revised Structure (v2.0):**
```
Phase 0: Technology Selection & Design (Days 1-3)
├── Dev 00: Database Selection ✅ COMPLETE
├── Dev 01: Design Document (NEXT)
├── Dev 02: Execution Plan
└── Gate 1: Design Review

Phase 1: Core Implementation (Days 4-9)
├── Dev 03-06: Core features

Phase 2: File Sync (Days 10-11)
├── Dev 07: File synchronization

Phase 3: Validation (Days 12-13)
├── Dev 08-09: Performance & convergence
└── Sci 03: Quality evaluation

Phase 4: Acceptance (Days 14-15)
└── Gate 4: Final decision
```

**Rationale:** 
- Clear phases with gates
- Proper dependency ordering
- Database choice happens first (foundational)
- Design happens after technology selection (concrete vs abstract)

---

## Research Tasks (Reduced from 9 to 6)

**Kept:**
- RT-1: Validate query latency (developer, during implementation)
- RT-2: Measure convergence time (developer, during implementation)
- RT-3: Choose vector database (developer, BLOCKING) → Now Dev 00 ✅ COMPLETE
- RT-5: Benchmark embedding model (developer, during implementation)
- RT-7: Measure write latency (developer, optimization)
- RT-10: Create evaluation test set (scientist, BLOCKING for quality) ✅ COMPLETE

**Removed:**
- RT-4: Dependency footprint (implementation detail)
- RT-6: POC comparison (duplicate of RT-3)
- RT-8: Chunking strategy (out of scope)
- RT-9: Query expansion (future feature)

**Rationale:** Focus on validation tasks, remove non-MVP items

---

## Quality Acceptance Criteria

**Primary Metric:** Precision@5 improvement over baseline
**Target:** > 30% relative improvement
**Validation:** Statistically significant (p < 0.05)

**Process:**
1. Scientist creates test set (50+ queries) ✅ Complete
2. Measure baseline (substring search) ✅ Complete
3. Measure semantic search (Dev 03-06)
4. Compare with paired t-test (Sci 03)

**Scientific Honesty:** If 30% not achieved, document actual improvement and make case for conditional acceptance (don't artificially adjust test set)

---

## Context Management System

**Decision Date:** 2025-12-16 morning

**Implementation:**
- Created MCP contexts for multi-session continuity
- Created cursor rule: `.cursor/rules/context-management.mdc`
- Contexts tracked in git (exception in `.gitignore`)
- Storage unlimited, retrieval < 10 contexts per session

**Rationale:**
- Maintain continuity across agent sessions
- Prevent context loss when switching sessions
- Enable team collaboration via git-tracked contexts
- Balance thoroughness (store everything) with efficiency (fetch selectively)

**Key Principles:**
- **Storage vs Retrieval:** Store liberally, fetch conservatively
- **Git is Ground Truth:** If context conflicts with git, trust git
- **Always Apply:** Context management rules apply to every session
- **Staleness Detection:** Check `last-updated`, cross-reference with git
- **Session Discipline:** Update current-status at session end
- **Atomicity:** Keep contexts < 200 lines, split when > 300 lines

---

## Evaluation Test Set Design (Scientist Action Item 01)

**Decision Date:** 2025-12-16 morning 
**Status:** ✅ Complete

**Decisions Made:**

1. **Test Set Size:** 55 queries (exceeds 50 minimum)
   - Rationale: Provides sufficient statistical power while remaining manageable

2. **Query Distribution:** 40/30/20/10 (how-to/factual/troubleshooting/comparison)
   - 22 how-to queries (40.0%)
   - 17 factual queries (30.9%)
   - 11 troubleshooting queries (20.0%)
   - 5 comparison queries (9.1%)
   - Rationale: Matches anticipated usage patterns for MCP context management

3. **Relevance Judgment Criteria:**
   - Binary relevance (relevant/not relevant)
   - Average 1.33 relevant contexts per query
   - No queries without relevant contexts
   - Rationale: Simple, clear criteria; sufficient for MVP validation

**Deliverables:**
- ✅ `evaluation-testset.json` (55 queries, all with relevance judgments)
- ✅ `evaluation-testset-methodology.md` (comprehensive documentation)

---

## Baseline Evaluation Design (Scientist Action Item 02)

**Decision Date:** 2025-12-16 afternoon  
**Status:** ✅ Complete

**Key Decisions:**

1. **Search Algorithm Choice: Word-Based Substring Search**
   - **NOT pure substring** (exact phrase matching → all metrics = 0.000)
   - **Word-based tokenization** with stop word filtering
   - **Rationale:** More realistic baseline, fairer comparison to semantic search

2. **Metrics Selected:**
   - Precision@5 (primary metric)
   - Recall@5 (ensure high recall maintained)
   - MRR (Mean Reciprocal Rank)
   - NDCG@10 (ranking quality)
   - **Rationale:** Standard IR metrics, matches requirements RT-10

**Baseline Results Achieved:**
- **Precision@5: 0.255** (25.5% of top-5 results relevant)
- **Recall@5: 0.945** (94.5% of relevant docs found)
- **MRR: 0.652** (relevant docs at positions 2-3 on average)
- **NDCG@10: 0.742** (good but improvable ranking)

**Target Metrics for Semantic Search (30% improvement):**
- **Precision@5 ≥ 0.332** (primary acceptance criterion)
- **Recall@5 ≥ 0.945** (maintain high recall)
- **MRR ≥ 0.848**
- **NDCG@10 ≥ 0.965**

**Deliverables:**
- ✅ `evaluate.py` - Reusable evaluation script
- ✅ `baseline-results.json` - Full per-query results
- ✅ `baseline-analysis.md` - Comprehensive analysis

---

## Summary

Phase 0 planning established:
- ✅ Revised workflow (database-first approach)
- ✅ Refined requirements (v2.0.0)
- ✅ Evidence-first methodology
- ✅ Clear MVP scope
- ✅ Locked technology constraints
- ✅ Phased action items structure
- ✅ Reduced research tasks
- ✅ Quality acceptance criteria
- ✅ Context management system
- ✅ Evaluation test set (55 queries)
- ✅ Baseline evaluation (P@5=0.255)
- ✅ Database choice (ChromaDB - see separate context)

All foundational decisions complete. Ready for Developer 01 (Design Document).

---

**References:**
- Critical database decision: `decision-database-chromadb`
- Current workflow: `action-items-revised`
- Current status: `current-status`
- Full requirements: `docs/v1/database/requirements.md`
